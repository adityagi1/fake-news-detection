unique(mpg$drv)
unique(mpg$cyl)
library(tidytext)
install.packages("tidytext")
library(tidytext)
library(janeaustenr)
library(tidyverse)
austen_books()
austen = austen_books()
View(austen)
text[5]
View(austen)
View(austen)
austen_books() %>% group_by(book)
grouped = austen %>% group_by(book)
View(grouped)
?str_detect
?cumsum()
orignal_books = austen_books() %>% group_by(book) %>% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case  = TRUE)))) %>% ungroup()
?str_detect
library(stringr)
orignal_books = austen_books() %>% group_by(book) %>% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case  = TRUE)))) %>% ungroup()
orignal_books
original_books = orignal_books
rm(orignal_books)
tidy_books = original_books %>% unnest_tokens(word, text)
tidy_books
?anti_join
tidy_books = tidy_books %>% anti_join(stop_words)
data("stop_words")
tidy_books = tidy_books %>% anti_join(stop_words)
tidy_books = tidy_books %>% anti_join(stop_words)
tidy_books
stop_words
tidy_books
count(tidy_books, word)
count(tidy_books, word, sort = TRUE)
library(tidyverse)
library(tidytext)
library(gutenbergr)
install.packages("gutenbergr")
library(gutenbergr)
hg_wells = gutenberg_download(c(35,36,5230,159))
hg_wells
View(hg_wells)
tidy_hgwells = hg_wells %>% unnest_tokens(word, text)
tidy_hgwells = tidy_hgwells %>% antijoin(stop_words)
library(dplyr)
tidy_hgwells = tidy_hgwells %>% antijoin(stop_words)
tidy_hgwells = tidy_hgwells %>% anti_join(stop_words)
stop_words
%>% hg_wells %>% unnest_tokens(word, text)
hg_wells %>% unnest_tokens(word, text)
tidy_hgwells
count(hg_wells, word)
count(tidy_hgwells, word)
count(tidy_hgwells, word, sort = TRUE)
bronte = gutenberg_download(c(1260, 768, 969, 9182, 767))
bronte
View(bronte)
tidy_bronte = bronte %>% unnest_tokens(word, text) %>% anti_join(stop_words)
count(tidy_bronte, word, sort = TRUE)
janeaustenr()
austen_books()
library(janeaustenr)
austen_books()
austen = austen_books()
mutate(tidy_bronte,author = "Bronte Sisters")
frequency = bind_rows(mutate(tidy_bronte, author = "Bronte Sisters"), mutate(tidy_hgwells, author = "HG Wells"))
frequency
unique(frequency$author)
unique(frequency$gutenberg_id)
frequency = frequency %>% str_extract(word = str_extract(word, "[a-z']+"))
library(stringr)
frequency = frequency %>% str_extract(word = str_extract(word, "[a-z']+"))
frequency = frequency %>% mutate(word = str_extract(word, "[a-z']+"))
frequency
frequency %>% count(author)
frequency %>% count(author, word)
frequency = frequency %>% count(author, word)
frequency
frequency %>% group_by(author)
a = frequency %>% group_by(author)
View(a)
View(frequency)
?group_by
rm(a)
frequency = frequency %>% group_by(author)
?dplyr
frequency %>% mutate(proportion = n/sum(n))
frequency = frequency %>% mutate(proportion = n/sum(n))
frequency
frequency = frequency %>% select(-n)
frequency
frequency %>% spread(author, proportion)
frequency = frequency %>% spread(author, proportion)
frequency %>% gather(author, proportion, "Bronte Sisters": "HG Wells")
frequency = frequency %>% gather(author, proportion, "Bronte Sisters": "HG Wells")
library(scales)
View(frequency)
ggplot(frequency, aes(x = proportion, y = "Jane Austen", color = abs("Jane Austen" - proportion))) +
geom_abline(color = "gray40", lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word, check_overlap = TRUE, vjust = 1.5)) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0,0.001), low = "darkslategrey4", high = "gray75")
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
library(tidyverse)
library(tidytext)
sentiments
get_sentiments('nrc')
get_sentiments('afinn')
get_sentiments('bing')
quit()
library(tidyverse)
library(tidytext)
lexicon
sentiments
get_sentiments("nrc")
get_sentiments("afinn")
get_sentiments("bing")
library(janeaustenr)
library(dplyr)
library(stirngr)
library(stringr)
austen_books()
tidy_books = austen_books() %>% group_by(book) %>% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("^chapter[\\divxlc]", ignore_case = TRUE)))) %>% ungroup() %>% unnest_tokens(word, text)
get_sentiments('nrc')
get_sentiments('nrc') %>% filter(sentiment == "joy")
nrcjoy = get_sentiments('nrc') %>% filter(sentiment == "joy")
tidy_books
tidy_books = tidy_books %>% select(word, everything())
tidy_books
?inner_join
tidy_books %>% filter(book = "Emma") %>% inner_join(nrcjoy, by = "word")
tidy_books %>% filter(book = "Emma") %>% inner_join(nrcjoy, "word")
tidy_books %>% filter(book == "Emma") %>% inner_join(nrcjoy, by = "word")
tidy_books %>% filter(book == "Emma") %>% inner_join(nrcjoy, by = "word") %>% count(word, sort = TRUE)
?table
?count
get_sentiments("bing")
get_sentiments("adinn")
get_sentiments("afinn")
get_sentiments("bing")
tidy_books()
tidy_books
tidy_books %>% inner_join(get_sentimens("bing"), by = "word")
tidy_books %>% inner_join(get_sentiments("bing"), by = "word")
tidy_books %>% inner_join(get_sentiments("bing"), by = "word") %>% count(book, index = linenumber %/% 80, sentiment)
tidy_books %>% inner_join(get_sentiments("bing"), by = "word") %>% count(book, index = linenumber %/% 80, sentiment) %>% spread(sentiment, n, fill = 0)
tidy_books %>% inner_join(get_sentiments("bing"), by = "word") %>% count(book, index = linenumber %/% 80, sentiment) %>% spread(sentiment, n, fill = 0) %>% mutate(sentiment = positive - negative)
janeaustensentiment = tidy_books %>% inner_join(get_sentiments("bing"), by = "word") %>% count(book, index = linenumber %/% 80, sentiment) %>% spread(sentiment, n, fill = 0) %>% mutate(sentiment = positive - negative)
ggplot(janeaustensentiment) + geom_col(mapping = aes(index, sentiment), show.legend = FALSE)
ggplot(janeaustensentiment) + geom_col(mapping = aes(index, sentiment, fill = book), show.legend = FALSE)
ggplot(janeaustensentiment) + geom_col(mapping = aes(index, sentiment, fill = book), show.legend = FALSE)  %>% facet_wrap(~book, ncol = 2, scales = "free_x")
ggplot(janeaustensentiment) + geom_col(mapping = aes(index, sentiment, fill = book), show.legend = FALSE)  + facet_wrap(~book, ncol = 2, scales = "free_x")
pride_predjudice = tidy_books %>% filter(book == "Pride & Prejudice")
pride_predjudice
afinn = pride_predjudice %>% inner_join(get_sentiments("afinn"), by = "word") %>% group_by(index = linenumber%/%80) %>% summarise(sentiment = sum(score)) %>% mutate(method = "AFINN")
afinn
bing_and_nrc = bind_rows(pride_predjudice %>% inner_join(get_sentiments("bing"), by = "word") %>% mutate(method = "Bing et al."), pride_prejudice %>% inner_join(get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative")),  by = "word") %>% mutate(method = "NRC")) %>% count(method, index = linenumber %/%80, sentiment) %>% spread(sentiment, n, fill = 0) %>% mutate(sentiment = positive - negative)
bing_and_nrc = bind_rows(pride_predjudice %>% inner_join(get_sentiments("bing"), by = "word") %>% mutate(method = "Bing et al."), pride_predjudice %>% inner_join(get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative")),  by = "word") %>% mutate(method = "NRC")) %>% count(method, index = linenumber %/%80, sentiment) %>% spread(sentiment, n, fill = 0) %>% mutate(sentiment = positive - negative)
pride_prejudice = pride_predjudice
rm(pride_predjudice)
get_sentiments("bing")
get_sentiments("nrc")
get_sentiments("nrc")$sentiment
unique(get_sentiments("nrc")$sentiment)
afinn
bing_and_nrc
unique(bing_and_nrc$method)
bind_rows(afinn, bing_and_nrc) %>% ggplot() %>% geom_col(aes(index, sentiment, fill = method))
ggplot(bind_rows(afinn, bing_and_nrc)) %>% geom_col(aes(index, sentiment, fill = method))
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method))
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method), show.legend = FALSE) + facet_wrap(~method, ncol = 1, scales = "free_y")
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method), show.legend = FALSE) + facet_wrap(~method, ncol = 1)
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method), show.legend = FALSE) + facet_wrap(~method, ncol = 1, scales = "free_y")
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method), show.legend = FALSE) + facet_wrap(~method, scales = "free_y")
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method), show.legend = FALSE) + facet_wrap(~method, nrows = 3, scales = "free_y")
ggplot(bind_rows(afinn, bing_and_nrc)) + geom_col(aes(index, sentiment, fill = method), show.legend = FALSE) + facet_wrap(~method, nrow = 3, scales = "free_y")
get_sentimens("bing")
get_sentiments("bing")
bing_word_counts = tidy_books $>$ inner_join(get_sentiments("bing")) %>% count(word, sentiment, sort = TRUE) %>% ungroup()
bing_word_counts = tidy_books %>% inner_join(get_sentiments("bing")) %>% count(word, sentiment, sort = TRUE) %>% ungroup()
bing_word_counts
bing_word_counts %>% filter(word == "miss")
bing_word_counts %>% filter(word == "happy")
bing_word_counts %>% filter(word == "love")
bing_word_counts
library(wordcloud)
tidy_books %>% anti_join(stop_words) %>% count(word)
tidy_books %>% anti_join(stop_words) %>% count(word)  %>% with(wordcloud(word, n, max.words = 1000))
tidy_books %>% anti_join(stop_words) %>% count(word)  %>% with(wordcloud(word, n, max.words = 100))
library(tidyverse), library(janeaustenr), library(tidytext)
library(tidyverse)
library(tidytext)
library(janeaustenr)
prideprejudice = austen_books() %>% filter(book == "Pride & Prejudice")
prideprejudice
prideprejudice %>% unnest_tokens(sentence, text, token = "sentence")
prideprejudice %>% unnest_tokens(sentence, text, token = "sentences")
prideprejudice %>% unnest_tokens(sentence, text, token = "sentences")$sentence[2]
data_frame(text = prideprejudice) %>% unnest_tokens(sentence, text, token = "sentences")
austen_books()
tidy_books = austen_books() %>% group_by(book) %>% mutate(linenumber = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>% ungroup() %>% unnest_tokens(word, text)
library(stringr)
tidy_books = austen_books() %>% group_by(book) %>% mutate(linenumber = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>% ungroup() %>% unnest_tokens(word, text)
tidy_books
rm(tidy_books)
tidy_books = austen_books() %>% group_by(book) %>% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>% ungroup() %>% unnest_tokens(word, text)
tidy_books
tidy_books = select(tidy_books, word, everything())
tidy_books
prideprejudice = tidy_books %>% filter(book = "Pride & Prejudice")
prideprejudice = tidy_books %>% filter(book == "Pride & Prejudice")
prideprejudice
austen_books()
austen_books()$text
prideprejudice
date_frame(text = prideprejudice)
data_frame(text = prideprejudice)
prideprejudice
pride_prejudice = prideprejudice
rm(prideprejudice)
pride_prejudice
prideprejudice
pride_prejudice
class(prideprejudice)
PandP_sentences = data_frame(text = prideprejudice) %>% unnest_tokens(sentence, text, token = "sentences")
PandP_sentences
PandP_sentences$sentence[2]
austen_chapters = austen_books() %>% group_by(book) %>% unnest_tokens(chapter, text, token = "regex", pattern = "Chapter|CHAPTER [\\dIVXLC]") %>% ungroup()
austen_chapters
austen_chapters %>% group_by(book) %>% summarise(chapters = n())
austen_chapters
austen_chapters$chapter
austen_chapters$chapter[0]
austen_chapters$chapter[1]
austen_chapters$chapter[2]
austen_chapters$chapter[5]
bingnegative = get_sentiments("bing") %>% filter(sentiment == "negative")
tidy_books %>% group_by(book, chapter) %>% summarise(n())
wordcounts = tidy_books %>% group_by(book, chapter) %>% summarise(n())
?semi_join
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(sadwords = n()) %>% inner_join(wordcounts, by = c("book", "chapters")) %>% mutate(ratio = sadwords/wordcounts)
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(sadwords = n()) %>% inner_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = sadwords/wordcounts)
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(sadwords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = sadwords/wordcounts)
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = sadwords/wordcounts)
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = negativewords/wordcounts)
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = negativewords/words)
wordcounts
?rename
rename(wordcounts, "n()", words)
rename(wordcounts, "n()", "words")
rename(wordcounts, words = n())
rename(wordcounts, words = "n()")
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = negativewords/words)
colnames(wordcounts)
wordcounts = rename(wordcounts, words = "n()")
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = negativewords/words)
tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = negativewords/words) %>% filter(chapter != 0) %>% top_n(1) %>% ungroup()
proportion_sad = tidy_books %>% semi_join(bingnegative) %>% group_by(book, chapter) %>% summarise(negativewords = n()) %>% left_join(wordcounts, by = c("book", "chapter")) %>% mutate(ratio = negativewords/words) %>% filter(chapter != 0) %>% top_n(1) %>% ungroup()
proportion_sad %>% sort("ratio")
?sort
install.packages("h2o")
library(h2o)
h2o.init()
load("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project/.RData")
View(reduced_data)
copy_reduced_data = reduced_data
str(reduced_data)
copy_webstats.train = webstats.train
copy_webstats.test = webstats.test
View(copy_webstats.train)
copy_webstats.train$global_rank = (",", "",copy_webstats.train$global_rank)
copy_webstats.train$global_rank = (",","", copy_webstats.train$global_rank)
copy_webstats.train$global_rank = ("\,","", copy_webstats.train$global_rank)
copy_webstats.train$global_rank = (",","",copy_webstats.train$global_rank)
copy_webstats.train$global_rank = ("\\,","",copy_webstats.train$global_rank)
copy_webstats.train$global_rank = (',', "" , copy_webstats.train$global_rank)
copy_webstats.train$global_rank = ('/,', "" , copy_webstats.train$global_rank)
copy_webstats.train$global_rank = ('\\,', "" , copy_webstats.train$global_rank)
copy_webstats.train$global_rank = ('[,]', "" , copy_webstats.train$global_rank)
View(copy_reduced_data)
write.csv("copy_reduced_data.csv")
?write.csv
write.csv(copy_reduced_data.csv, "copy_reduced_data.csv")
write.csv(copy_reduced_data, "copy_reduced_data.csv")
setwd("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project")
write.csv(copy_reduced_data, "copy_reduced_data.csv")
copy_reduced_data = csv.read("copy_reduced_data.csv", stringsAsFactors = TRUE)
copy_reduced_data = read.csv("copy_reduced_data.csv", stringsAsFactors = TRUE)
View(copy_reduced_data)
copy_reduced_data = copy_reduced_data %>% select(-X)
library(tidyverse)
copy_reduced_data = copy_reduced_data %>% select(-X)
View(copy_reduced_data)
str(copy_reduced_data)
set.seed(1008)
library(caTools)
split = sample.split(reduced_data$type, SplitRatio = 0.7)
train = filter(reduced_data, split == TRUE)
test = filter(reduced_data, split == FALSE)
socialmedia.train = train %>% select(ID, site_url, stumble_upon, pinterest, linkedin, fb_total, fb_comments, fb_reactions, fb_shares, type)
socialmedia.test = test %>% select(ID, site_url, stumble_upon, pinterest, linkedin, fb_total, fb_comments, fb_reactions, fb_shares, type)
webstats.train = train %>% select(ID, site_url, author, language, country, global_rank, US_rank, top1_country, top2_country, top3_country, bounce_rate, daily_pageviews_per_visitor, daily_time_on_site, percentage_search_visits, num_sites_linking_in, gender, educational_level, browsing_location, type)
webstats.test = test %>% select(ID, site_url, author, language, country, global_rank, US_rank, top1_country, top2_country, top3_country, bounce_rate, daily_pageviews_per_visitor, daily_time_on_site, percentage_search_visits, num_sites_linking_in, gender, educational_level, browsing_location, type)
nlp.train = train %>% select(ID, site_url, headline, sentiment, gop_nlp, email_nlp, comei_nlp, reveal_nlp, comment_nlp, soro_nlp, acknowledg_nlp, emf_nlp, rig_nlp, jihad_nlp, femin_nlp, hillary_nlp, neomasculin_nlp, natur_nlp, weight_nlp, gmo_nlp, trump_nlp, brief_nlp, roi_nlp, tax_nlp,type)
nlp.test = test %>% select(ID, site_url, headline, sentiment, gop_nlp, email_nlp, comei_nlp, reveal_nlp, comment_nlp, soro_nlp, acknowledg_nlp, emf_nlp, rig_nlp, jihad_nlp, femin_nlp, hillary_nlp, neomasculin_nlp, natur_nlp, weight_nlp, gmo_nlp, trump_nlp, brief_nlp, roi_nlp, tax_nlp,type)
split = sample.split(copy_reduced_data$type, SplitRatio = 0.7)
train = filter(copy_reduced_data, split == TRUE)
test = filter(copy_reduced_data, split == FALSE)
socialmedia.train = train %>% select(ID, site_url, stumble_upon, pinterest, linkedin, fb_total, fb_comments, fb_reactions, fb_shares, type)
socialmedia.test = test %>% select(ID, site_url, stumble_upon, pinterest, linkedin, fb_total, fb_comments, fb_reactions, fb_shares, type)
webstats.train = train %>% select(ID, site_url, author, language, country, global_rank, US_rank, top1_country, top2_country, top3_country, bounce_rate, daily_pageviews_per_visitor, daily_time_on_site, percentage_search_visits, num_sites_linking_in, gender, educational_level, browsing_location, type)
webstats.test = test %>% select(ID, site_url, author, language, country, global_rank, US_rank, top1_country, top2_country, top3_country, bounce_rate, daily_pageviews_per_visitor, daily_time_on_site, percentage_search_visits, num_sites_linking_in, gender, educational_level, browsing_location, type)
nlp.train = train %>% select(ID, site_url, headline, sentiment, gop_nlp, email_nlp, comei_nlp, reveal_nlp, comment_nlp, soro_nlp, acknowledg_nlp, emf_nlp, rig_nlp, jihad_nlp, femin_nlp, hillary_nlp, neomasculin_nlp, natur_nlp, weight_nlp, gmo_nlp, trump_nlp, brief_nlp, roi_nlp, tax_nlp,type)
nlp.test = test %>% select(ID, site_url, headline, sentiment, gop_nlp, email_nlp, comei_nlp, reveal_nlp, comment_nlp, soro_nlp, acknowledg_nlp, emf_nlp, rig_nlp, jihad_nlp, femin_nlp, hillary_nlp, neomasculin_nlp, natur_nlp, weight_nlp, gmo_nlp, trump_nlp, brief_nlp, roi_nlp, tax_nlp,type)
str(webstats.train$global_rank)
str(webstats.train$daily_time_on_site)
save.image("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project/.RData")
str(webstats.train$daily_pageviews_per_visitor)
str(webstats.train)
str(reduced_data)
rm(copy_webstats.test)
rm(copy_webstats.train)
save.image("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project/.RData")
View(webstats.train)
rf_ws_model = randomForest(type ~ author + language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train, mtry = 5, nodesize = 5, ntree = 500)
library(randomForest)
rf_ws_model = randomForest(type ~ author + language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train, mtry = 5, nodesize = 5, ntree = 500)
str(webstats.train)
rf_ws_model = randomForest(type ~ language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train, mtry = 5, nodesize = 5, ntree = 500)
rf_ws_pred = predict(rf_ws_model, newdata = webstats.test)
table(webstats.test$type, rf_ws_pred)
(87 + 390 + 78 + 60 + 29 + 391)/nrow(webstats.test)
library(rpart)
cart_ws_model = rpart(type ~ language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train, method = "class", minbucket = 5, cp = 0.001)
cart_ws_pred = predict(cart_ws_model, newdata = webstats.test, type = "class")
table(webstats.test$type, cart_ws_pred)
(87 + 390 + 76 + 60 + 29 + 391)/nrow(webstats.test)
library(e1071)
svm_ws_model = svm(type ~ language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train, cost = 100, gamma = 1)
svm_ws_pred = predict(svm_ws_model, newdata = webstats.test)
table(webstats.test$type, svm_ws_pred)
(87 + 390 + 78 + 60 + 29 + 391)/nrow(webstats.test)
(87 + 390 + 76 + 60 + 29 + 391)/nrow(webstats.test)
library(MASS)
lda_ws_model = lda(type ~ language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train)
lda_ws_model = lda(type ~ language + global_rank + US_rank + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train)
lda_ws_model = lda(type ~ language + country + global_rank + US_rank + top1_country + top2_country + top3_country + bounce_rate + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train)
lda_ws_model = lda(type ~ language + country + global_rank + US_rank + top2_country + top3_country + daily_pageviews_per_visitor + daily_time_on_site + percentage_search_visits + num_sites_linking_in + gender + educational_level + browsing_location, data = webstats.train)
knn_ws_pred = knn(train = webstats.train, test = webstats.test, webstats.train$type)
library(class)
knn_ws_pred = knn(train = webstats.train, test = webstats.test, webstats.train$type)
knn_ws_pred = knn(train = webstats.train, test = webstats.test, webstats.train$type, k = 5)
library(h2o)
h2o.init()
y = "response"
?set_diff
?setdiff
x = setdiff(names(train), y)
train_h2o =  train
test_h2o = test
nfolds = 5
?h2o.randomForest
View(nlp.train)
View(copy_reduced_data)
str(train)
str(test)
my_rf = h2o.randomForest(c("sentiment", "gop_nlp", "comei_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledge_nlp", "emf_nlp", "rig_nlp", "jihad_nlp","femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp","trump_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, ntrees = 500, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, seed = 108)
rm(train_h2o)
as.h2o(train, "train_h2o")
save.image("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project/.RData")
typeof(as.h2o(train, "train_h2o"))
class(as.h2o(train, "train_h2o"))
train_h2o = as.h2o(train, "train_h2o"))
train_h2o = as.h2o(train, "train_h2o")
test_h2o = as.h2o(test, 'test_h2o')
my_rf = h2o.randomForest(c("sentiment", "gop_nlp", "comei_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledge_nlp", "emf_nlp", "rig_nlp", "jihad_nlp","femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp","trump_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, ntrees = 500, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, seed = 108)
my_rf = h2o.randomForest(c("sentiment", "gop_nlp", "comei_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledg_nlp", "emf_nlp", "rig_nlp", "jihad_nlp","femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp","trump_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, ntrees = 500, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, seed = 108)
my_rf = h2o.gbm(c("stumble_upon", "pinterest", "linkedin", "fb_total", "fb_comments", "fb_reactions", "fb_shares"), "type", training_frame = train_h2o,ntrees = 100, max_depth = 3, min_rows = 10, learn_rate = 0.001, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, seed = 108)
nba_nlp_model = naiveBayes(type ~ sentiment + gop_nlp + email_nlp + reveal_nlp + comment_nlp + soro_nlp + acknowledg_nlp + emf_nlp + rig_nlp + jihad_nlp + femin_nlp + hillary_nlp + neomasculin_nlp + natur_nlp + weight_nlp + gmo_nlp + trump_nlp + brief_nlp + roi_nlp + tax_nlp, data = nlp.train)
nba_nlp_pred = predict(nba_nlp_model, newdata = nlp.test)
table(nlp.test$type,nba_nlp_pred)
(86+15)/nrow(nlp.test)
install.packages("nnet")
library(nnet)
log_nlp_model = multinom(type ~ sentiment + gop_nlp + email_nlp + reveal_nlp + comment_nlp + soro_nlp + acknowledg_nlp + emf_nlp + rig_nlp + jihad_nlp + femin_nlp + hillary_nlp + neomasculin_nlp + natur_nlp + weight_nlp + gmo_nlp + trump_nlp + brief_nlp + roi_nlp + tax_nlp, data = nlp.train)
summary(log_nlp_model)
log_nlp_pred = predict(log_nlp_model, newdata = nlp.test)
table(nlp.test, log_nlp_pred)
log_nlp_pred
class(log_nlp_pred)
class(nlp.test$type)
table(nlp.test, unlist(log_nlp_pred))
yes
str(log_nlp_pred)
typeof(log_nlp_pred)
typeof(nlp.test$type)
log_nlp_pred
as_data_frame(cbind(nlp.test$type, log_nlp_pred))
View(nlp.test)
ac = as_data_frame(cbind(nlp.test$type, log_nlp_pred))
ac = ac %>% mutate(correct = ac[1] == ac[2])
ac$correct = ac[1] == ac[2]
ac
as$correct = as$correct %>% as.numeric()
sum(as$correct)
table(as$correct)
class(as$correct)
ac$correct = ac$correct %>% as.numeric()
ac
sum(ac$correct)
sum(ac$correct)/nrow(ac)
?h2o.glm
my_glm = h2o.glm(c("sentiment", "gop_nlp", "comei_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledge_nlp", "emf_nlp", "rig_nlp", "jihad_nlp","femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp","trump_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, nfolds = 5, seed = 108, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, family = "multinomial")
my_glm = h2o.glm(c("sentiment", "gop_nlp", "comei_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledg_nlp", "emf_nlp", "rig_nlp", "jihad_nlp","femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp","trump_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, nfolds = 5, seed = 108, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, family = "multinomial")
my_rf
my_gbm = h2o.gbm(c("stumble_upon", "pinterest", "linkedin", "fb_total", "fb_comments", "fb_reactions", "fb_shares"), "type", training_frame = train_h2o,ntrees = 100, max_depth = 3, min_rows = 10, learn_rate = 0.001, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, seed = 108)
my_rf = h2o.randomForest(c("language", "country", "global_rank", "US_rank", "top1_country", "top2_country", "top3_country", "bounce_rate", "daily_pageviews_per_visitor", "daily_time_on_site", "percentage_search_visits", "num_sites_linking_in", "gender", "educational_level", "browsing_location"), "type", training_frame = train_h2o, ntrees = 500, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE, seed = 108)
my_ensemble = h2o.stackedEnsemble(c("language", "country", "global_rank", "US_rank", "top1_country", "top2_country", "top3_country", "bounce_rate", "daily_pageviews_per_visitor", "daily_time_on_site", "percentage_search_visits", "num_sites_linking_in", "gender", "educational_level", "browsing_location", "stumble_upon","pinterest", "linkedin", "fb_total", fb_comments", "fb_reactions", "fb_shares", "sentiment", "gop_nlp", "email_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledg_nlp", "emf_nlp", "rig_nlp", "jihad_nlp", "femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, base_models = list(my_glm, my_rf, my_gbm))
my_ensemble = h2o.stackedEnsemble(c("language", "country", "global_rank", "US_rank", "top1_country", "top2_country", "top3_country", "bounce_rate", "daily_pageviews_per_visitor", "daily_time_on_site", "percentage_search_visits", "num_sites_linking_in", "gender", "educational_level", "browsing_location", "stumble_upon","pinterest", "linkedin", "fb_total", "fb_comments", "fb_reactions", "fb_shares", "sentiment", "gop_nlp", "email_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledg_nlp", "emf_nlp", "rig_nlp", "jihad_nlp", "femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, base_models = list(my_glm, my_rf, my_gbm))
perf = h2o.performance(my_ensemble, newdata = test_h2o)
pred = h2o.predict(ensemble, newdata = test_h2o)
pred = h2o.predict(my_ensemble, newdata = test_h2o)
table(test$type, pred)
length(pred)
pred
?h2o.predict()
my_ensemble = h2o.stackedEnsemble(c("language", "country", "global_rank", "US_rank", "top1_country", "top2_country", "top3_country", "bounce_rate", "daily_pageviews_per_visitor", "daily_time_on_site", "percentage_search_visits", "num_sites_linking_in", "gender", "educational_level", "browsing_location", "stumble_upon","pinterest", "linkedin", "fb_total", "fb_comments", "fb_reactions", "fb_shares", "sentiment", "gop_nlp", "email_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledg_nlp", "emf_nlp", "rig_nlp", "jihad_nlp", "femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, base_models = list(my_glm, my_rf, my_gbm), family = "multinomial")
my_ensemble = h2o.stackedEnsemble(c("language", "country", "global_rank", "US_rank", "top1_country", "top2_country", "top3_country", "bounce_rate", "daily_pageviews_per_visitor", "daily_time_on_site", "percentage_search_visits", "num_sites_linking_in", "gender", "educational_level", "browsing_location", "stumble_upon","pinterest", "linkedin", "fb_total", "fb_comments", "fb_reactions", "fb_shares", "sentiment", "gop_nlp", "email_nlp", "reveal_nlp", "comment_nlp", "soro_nlp", "acknowledg_nlp", "emf_nlp", "rig_nlp", "jihad_nlp", "femin_nlp", "hillary_nlp", "neomasculin_nlp", "natur_nlp", "weight_nlp", "gmo_nlp", "brief_nlp", "roi_nlp", "tax_nlp"), "type", training_frame = train_h2o, base_models = list(my_glm, my_rf, my_gbm))
pred = h2o.predict(my_ensemble, newdata = test_h2o)
pred
table(test$type, pred$predict)
pred$predict
test$type
length(test$type)
length(pred$predict)
class(pred$predict)
ensemble_pred_r = as.data.frame(pred)
ensemble_pred_r
class(ensemble_pred_r)
class(ensemble_pred_r$predict)
table(test$type, ensemble_pred_r$predict)
savehistory("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project/.RData")
perf_ensemble_test <- h2o.performance(ensemble, newdata = test)
perf_ensemble_test <- h2o.performance(my_ensemble, newdata = test)
perf_ensemble_test <- h2o.performance(my_ensemble, newdata = test_h2o)
perf_gbm_test <- h2o.performance(my_gbm, newdata = test_h2o)
perf_rf_test <- h2o.performance(my_rf, newdata = test_h2o)
perf_glm_test <- h2o.performance(my_glm, newdata = test_h2o)
baselearner_best_auc_test <- max(h2o.auc(perf_gbm_test), h2o.auc(perf_rf_test), h2o.auc(perf_glm_test))
ensemble_auc_test <- h2o.auc(perf_ensemble_test)
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
perf_ensemble_test
perf_ensemble_test$auc
perf_ensemble_test.auc
savehistory("C:/Users/Aditya Tyagi/Desktop/Aditya Tyagi (Original)/UC Berkeley Work (3rd Year)/Fall Semester/IEOR142 - Machine Learning & Data Analytics/Course Project/.RData")
reduced_data %>% filter(site_url == "nytimes.com")
length(reduced_data %>% filter(site_url == "nytimes.com"))
View(socialmedia)
View(alexa)
h2o.performance(my_gbm, newdata = test_h2o)
h2o.auc(h2o.performance(my_gbm, newdata = test_h2o))
h2o.auc(h2o.performance(my_rf, newdata = test_h2o))
View(reduced_data)
perf_ensemble_test
ggplot(copy_reduced_data) %>% + geom_bar(aes(x = type))
ggplot(copy_reduced_data) %>% + geom_bar(aes(x = type), color = "green")
ggplot(copy_reduced_data) %>% + geom_bar(aes(x = type), fill = "green")
ggplot(copy_reduced_data) %>% + geom_bar(aes(x = type), fill = "blue")
ggscatmat(copy_reduced_data, columns = 12:16,20:26, 27)
library(car)
ggscatmat(copy_reduced_data, columns = 12:16,20:26, 27)
library(ggplot2)
ggscatmat(copy_reduced_data, columns = 12:16,20:26, 27)
library(GGally)
ggscatmat(copy_reduced_data, columns = 12:16,20:26, 27)
ggscatmat(copy_reduced_data, columns = 12:16, alpha = 0.8)
ggscatmat(copy_reduced_data, columns = 12:16, alpha = 0.8, color = "purple")
ggscatmat(copy_reduced_data, columns = 12:16, alpha = 0.8, color = "blue")
View(copy_reduced_data)
ggscatmat(copy_reduced_data, columns = 12:16, alpha = 0.8, color = "type")
ggscatmat(copy_reduced_data, columns = 12:16, alpha = 0.8)
ggscatmat(copy_reduced_data, columns = 12:16, alpha = 0.8, color = "type")
ggscatmat(copy_reduced_data, columns = 20:26, alpha = 0.8, color = "type")
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment))
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment), binwidth = 15)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment), binwidth = 2)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment), binwidth = 1)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment), binwidth = 5)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment), binwidth = 3)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment), binwidth = 2)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment, fill = type), binwidth = 2)
+ facet_wrap(~type)
+ facet_wrap(type ~ sentiment)
ggplot(reduced_data) %>% + geom_histogram(aes(x=sentiment, fill = type), binwidth = 2) + facet_wrap(~type)
library(wordcloud)
wordcloud(reduced_data %>% filter(type == "BS"))
wordcloud(reduced_data %>% filter(type == "BS"))
write.csv(copy_reduced_data, "reduced_data.csv")
